{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2612bed0",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6694335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f7f01",
   "metadata": {},
   "source": [
    "### Etapas:\n",
    "\n",
    "1. Inicialização dos pesos e bias\n",
    "2. FeedForward\n",
    "3. Calculo da perda\n",
    "4. Backpropagation\n",
    "5. Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a48b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# ### Etapas:\n",
    "# \n",
    "# 1. Inicialização dos pesos e bias\n",
    "# 2. FeedForward\n",
    "# 3. Calculo da perda\n",
    "# 4. Backpropagation\n",
    "# 5. Fit\n",
    "\n",
    "# %%\n",
    "class RnModel:\n",
    "\n",
    "  def __init__(self, x: np.ndarray, y: np.ndarray, hidden_neurons: int = 10, \n",
    "               output_neurons: int = 1, random_seed: int | None = None, activation: str = 'sigmoid'):\n",
    "    \"\"\"\n",
    "      Inicializa uma rede neural simples com uma camada oculta.\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      x : np.ndarray\n",
    "          Matriz de entrada com formato (n_amostras, n_features).\n",
    "      y : np.ndarray\n",
    "          Vetor ou matriz de rótulos com formato (n_amostras,) ou (n_amostras, n_outputs).\n",
    "      hidden_neurons : int\n",
    "          Número de neurônios na camada oculta.\n",
    "      output_neurons : int\n",
    "          Número de neurônios na camada de saída.\n",
    "      random_seed : int | None\n",
    "          Valor para reprodutibilidade da inicialização aleatória dos pesos.\n",
    "      activation : str\n",
    "          Nome da função de ativação usada na camada oculta. \n",
    "          Opções: 'tanh', 'relu', 'sigmoid'.\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Este construtor configura a estrutura básica da rede:\n",
    "        - Valida as dimensões de entrada.\n",
    "        - Define os tamanhos das camadas.\n",
    "        - Inicializa pesos e bias com pequenas variações aleatórias (método Xavier).\n",
    "        - Define a função de ativação escolhida em activation.\n",
    "        - Armazena parâmetros principais e histórico de perda.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validações simples\n",
    "    if x.ndim != 2:\n",
    "      raise ValueError(\"x deve ser uma matriz 2D com shape (n_amostras, n_features)\")\n",
    "    if y.ndim not in (1, 2):\n",
    "      raise ValueError(\"y deve ser um vetor 1D ou matriz 2D de rótulos\")\n",
    "\n",
    "    self.x = x\n",
    "    # garantir formato coluna para y quando saída única\n",
    "    self.y = y.reshape(-1, 1) if (y.ndim == 1 and output_neurons == 1) else y\n",
    "\n",
    "    self.hidden_neurons = int(hidden_neurons)\n",
    "    self.output_neurons = int(output_neurons)\n",
    "    \n",
    "    # dimensões que é o Nº de entradas é o numero de entradas (colunas na entrada x)\n",
    "    self.input_neurons = x.shape[1]\n",
    "\n",
    "    # inicialização de pesos (pequenos valores aleatórios) e biases (zeros)\n",
    "    # Xavier Inicialization -> Variancia dos pesos igual em todas as camadas\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    # Camada de entrada para a camada oculta\n",
    "    self.w1 = rng.randn(self.input_neurons, self.hidden_neurons) / np.sqrt(self.input_neurons)  # (n_features, hidden)\n",
    "    self.b1 = np.zeros((1, self.hidden_neurons)) # (1, hidden)\n",
    "    \n",
    "    # Camada oculta para a camada de saída                     \n",
    "    self.w2 = rng.randn(self.hidden_neurons, self.output_neurons) / np.sqrt(self.hidden_neurons)  # (hidden, output)\n",
    "    self.b2 = np.zeros((1, self.output_neurons))  # (1, output)\n",
    "    self.z1 = 0\n",
    "    self.f1 = 0\n",
    "    self.activation = activation.lower()\n",
    "\n",
    "    self.model_dist = {\n",
    "      'w1': self.w1,\n",
    "      'b1': self.b1,\n",
    "      'w2': self.w2,\n",
    "      'b2': self.b2,\n",
    "      'random_seed': random_seed,\n",
    "      'activation': self.activation\n",
    "    }\n",
    "\n",
    "    # histórico de perda\n",
    "    self.loss_history = []\n",
    "\n",
    "  # -------------------- Funções auxiliares --------------------\n",
    "\n",
    "  def showPlot(self, predictions):\n",
    "    \"\"\"\n",
    "      Plota um scatter 2D das amostras coloridas pelas predições.\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      predictions : np.ndarray\n",
    "          Vetor de rótulos ou previsões com comprimento igual ao número de amostras (n_amostras,).\n",
    "\n",
    "      Retorna:\n",
    "      --------\n",
    "      None\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Exibe um gráfico de dispersão usando as duas primeiras características de entrada\n",
    "      (x[:, 0] e x[:, 1]) e colore cada ponto de acordo com 'predictions'. A função\n",
    "      exige que os dados de entrada tenham exatamente 2 features.\n",
    "    \"\"\"\n",
    "    if self.x.shape[1] != 2:\n",
    "      raise ValueError(\"showPlot só funciona para entradas 2D com shape (n_amostras, 2)\")\n",
    "\n",
    "    plt.scatter(self.x[:, 0], self.x[:, 1], s=50, c=predictions, cmap='rainbow', alpha=0.7)\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.title('Predições (showPlot)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  def activation_forward(self, z):\n",
    "    \"\"\"Aplica a função de ativação selecionada.\"\"\"\n",
    "    if self.activation == \"tanh\":\n",
    "      return np.tanh(z)\n",
    "    elif self.activation == \"relu\":\n",
    "      return np.maximum(0, z)\n",
    "    elif self.activation == \"sigmoid\":\n",
    "      return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "      raise ValueError(f\"Função de ativação '{self.activation}' não suportada.\")\n",
    "      \n",
    "  # ------------------------------------------------------------\n",
    "\n",
    "  def activation_derivative(self, z):\n",
    "    \"\"\"Calcula a derivada da função de ativação selecionada.\"\"\"\n",
    "    if self.activation == \"tanh\":\n",
    "      return (1 - np.power(np.tanh(z), 2))\n",
    "    elif self.activation == \"relu\":\n",
    "      return (z > 0).astype(float)\n",
    "    elif self.activation == \"sigmoid\":\n",
    "      s = 1 / (1 + np.exp(-z))\n",
    "      return s * (1 - s)\n",
    "    else:\n",
    "      raise ValueError(f\"Função de ativação '{self.activation}' não suportada.\")\n",
    "\n",
    "  def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "      Realiza a etapa de propagação direta (forward pass).\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      x : np.ndarray\n",
    "          Entradas da rede com formato (n_amostras, n_features).\n",
    "\n",
    "      Retorna:\n",
    "      --------\n",
    "      np.ndarray\n",
    "          Saída da rede após a função activations escolhida .\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Executa o fluxo de dados da entrada até a saída:\n",
    "        1. Calcula z1 = x·w1 + b1.\n",
    "        2. Aplica a função de ativação tanh: f1 = tanh(z1).\n",
    "        3. Calcula z2 = f1·w2 + b2.\n",
    "        4. Aplica a função softmax para converter as saídas em probabilidades.\n",
    "    \"\"\"\n",
    "    # Equação da reta (1)\n",
    "    # Multiplicação de matrizes + bias \n",
    "    self.z1 = x.dot(self.w1) + self.b1\n",
    "\n",
    "    # Função de ativação (1)\n",
    "    self.f1 = self.activation_forward(self.z1)\n",
    "\n",
    "    # Equação da reta (2)\n",
    "    z2 = self.f1.dot(self.w2) + self.b2\n",
    "    \n",
    "    # Se for problema binário (1 neurônio na saída), use sigmoid\n",
    "    if self.output_neurons == 1:\n",
    "        return self.activation_forward(z2)\n",
    "    else:\n",
    "    # Softmax (Probabilidade de cada classe) -> Função de ativação (2)\n",
    "        exp_values = np.exp(z2)\n",
    "        softmax = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        return softmax\n",
    "\n",
    "\n",
    "  def loss(self, output):\n",
    "    \"\"\"\n",
    "      Calcula a função de perda (erro) utilizando entropia cruzada.\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      output : np.ndarray\n",
    "          Saídas previstas da rede (probabilidades de cada classe).\n",
    "\n",
    "      Retorna:\n",
    "      --------\n",
    "      float\n",
    "          Valor médio da perda para a época atual.\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Mede o erro entre as previsões do modelo e os rótulos corretos.\n",
    "      A função de entropia cruzada penaliza previsões com baixa probabilidade\n",
    "      para as classes verdadeiras.\n",
    "    \"\"\"\n",
    "    if self.output_neurons == 1:\n",
    "        # Binary Cross-Entropy\n",
    "        eps = 1e-9\n",
    "        return -np.mean(self.y * np.log(output + eps) + (1 - self.y) * np.log(1 - output + eps))\n",
    "    else:\n",
    "        # Categorical Cross-Entropy\n",
    "        predictions = np.zeros(self.y.shape[0])\n",
    "        for i, correct_index in enumerate(self.y.astype(int).ravel()):\n",
    "            predictions[i] = output[i][correct_index]\n",
    "        log_prob = -np.log(predictions + 1e-9)\n",
    "        return np.mean(log_prob)\n",
    "\n",
    "\n",
    "  def backpropagation(self, softmax: np.ndarray, learning_rate: float = 0.1):\n",
    "    \"\"\"\n",
    "      Realiza o processo de retropropagação do erro (backpropagation).\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      softmax : np.ndarray\n",
    "          Saídas obtidas da etapa de forward.\n",
    "      learning_rate : float\n",
    "          Taxa de aprendizado usada para ajustar os pesos.\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Calcula os gradientes do erro em relação aos pesos e bias,\n",
    "      e ajusta os parâmetros do modelo com base no gradiente descendente:\n",
    "        - Calcula os deltas das camadas (delta2 e delta1).\n",
    "        - Obtém gradientes dw e db.\n",
    "        - Atualiza pesos e bias com o fator de aprendizado definido.\n",
    "    \"\"\"\n",
    "    # Cópia da saída do softmax\n",
    "    delta2 = np.copy(softmax)\n",
    "    \n",
    "    # Corrigido: cálculo correto da diferença entre saída prevista e real\n",
    "    delta2[range(self.x.shape[0]), self.y] -= 1\n",
    "\n",
    "    # Gradientes da camada de saída\n",
    "    dw2 = (self.f1.T).dot(delta2)\n",
    "    db2 = np.sum(delta2, axis = 0, keepdims=True)\n",
    "\n",
    "    # Gradientes da camada oculta (derivada da tanh)\n",
    "    delta1 = delta2.dot(self.w2.T) * self.activation_derivative(self.z1)\n",
    "    dw1 = (self.x.T).dot(delta1)\n",
    "    db1 = np.sum(delta1, axis = 0, keepdims=True)\n",
    "\n",
    "    # Atualização dos pesos e bias\n",
    "    self.w1 += -learning_rate * dw1  # 'w1 = w1 - self.learning_rate*dw1'\n",
    "    self.w2 += -learning_rate * dw2 \n",
    "    self.b1 += -learning_rate * db1 \n",
    "    self.b2 += -learning_rate * db2 \n",
    "    \n",
    "\n",
    "\n",
    "  def fit(self, learning_rate: float = 0.1, epochs: int = 1000):\n",
    "    \"\"\"\n",
    "      Treina o modelo usando o algoritmo de descida do gradiente (Gradient Descent).\n",
    "\n",
    "      Parâmetros:\n",
    "      ----------\n",
    "      learning_rate : float\n",
    "          Taxa de aprendizado que define o tamanho do passo na atualização dos pesos.\n",
    "      epochs : int\n",
    "          Número de iterações (épocas) de treinamento.\n",
    "\n",
    "      Retorna:\n",
    "      --------\n",
    "      np.ndarray\n",
    "          Predições finais do modelo após o término do treinamento.\n",
    "\n",
    "      Descrição:\n",
    "      ----------\n",
    "      Para cada época:\n",
    "        1. Executa a propagação direta (forward).\n",
    "        2. Calcula a perda média (erro).\n",
    "        3. Atualiza pesos e bias pela retropropagação.\n",
    "        4. Mede a acurácia com base nas predições atuais.\n",
    "      Exibe periodicamente métricas de desempenho.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "      outputs = self.forward(self.x)\n",
    "      loss = self.loss(outputs)\n",
    "      self.loss_history.append(loss)\n",
    "      self.backpropagation(outputs, learning_rate)\n",
    "\n",
    "      # Calculo de acuracia\n",
    "      y_true = self.y.ravel()\n",
    "      if self.output_neurons == 1:\n",
    "        prediction = (outputs > 0.5).astype(int).ravel()\n",
    "      else:\n",
    "        prediction = np.argmax(outputs, axis=1)\n",
    "      correct = np.sum(prediction == y_true)\n",
    "      accuracy = correct / len(y_true)\n",
    "      \n",
    "\n",
    "      # Correção na condição de exibição de progresso\n",
    "      if (epoch + 1) % max(1, (epochs // 10)) == 0:\n",
    "        print(f'Epoch: [{epoch+1} / {epochs}] Accuracy: {accuracy:.3f} Loss: {loss:.4f} Correct: {correct} Total: {self.y.shape[0]}')\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 10 # 10 neuronios na camada intermediaria/oculta\n",
    "output_neurons = 4 # Agora são 4 classes, ou seja 4 neuronios na camada de saida\n",
    "random_seed = 8 \n",
    "learning_rate = 0.001 # taxa de aprendizado\n",
    "epochs = 500 # epocas de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c12eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "modelo = RnModel(x, y, hidden_neurons=hidden_neurons, output_neurons=output_neurons, random_seed=random_seed)\n",
    "result2 = modelo.fit(learning_rate=learning_rate, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd9b7e",
   "metadata": {},
   "source": [
    "#### Para salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c88deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "  pickle.dump(modelo, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e8e79",
   "metadata": {},
   "source": [
    "#### Para carregar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as f:\n",
    "  loaded_model = pickle.load(f)\n",
    "  # Exemplo de uso\n",
    "  loaded_model.showPlot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
